import json
import os
import shutil
from pathlib import Path

from bs4 import BeautifulSoup
from dotenv import load_dotenv

from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()

# Paths
CHROMA_PATH = "data/chroma_db"
DATA_FILE_JSONL = Path("data/raw_docs.jsonl")
DATA_FILE_JSON = Path("data/raw_docs.json")
CHUNKS_FILE = Path("data/chunks.jsonl")  # <-- Hybrid (BM25) áƒáƒ›áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡

# Chunking
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200

# Skip tiny docs
MIN_CONTENT_LEN = 60


def clean_html(html_text: str) -> str:
    if not html_text:
        return ""
    soup = BeautifulSoup(html_text, "lxml")
    return soup.get_text(separator="\n", strip=True)


def load_raw_docs() -> list:
    """
    áƒ™áƒ˜áƒ—áƒ®áƒ£áƒšáƒáƒ‘áƒ¡ JSONL-áƒ¡ áƒ—áƒ£ áƒáƒ áƒ¡áƒ”áƒ‘áƒáƒ‘áƒ¡, áƒ—áƒ£ áƒáƒ áƒ â€” JSON-áƒ¡.
    """
    if DATA_FILE_JSONL.exists():
        docs = []
        with DATA_FILE_JSONL.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    docs.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
        return docs

    if DATA_FILE_JSON.exists():
        try:
            return json.loads(DATA_FILE_JSON.read_text(encoding="utf-8"))
        except Exception:
            return []

    return []


def export_chunks_jsonl(chunks: list[Document], out_path: Path):
    """
    BM25Retriever-áƒ¡áƒ—áƒ•áƒ˜áƒ¡ áƒ•áƒ˜áƒ¬áƒ”áƒ áƒ— chunk-áƒ”áƒ‘áƒ¡ áƒ¤áƒáƒ˜áƒšáƒáƒ“ (JSONL).
    """
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with out_path.open("w", encoding="utf-8") as f:
        for d in chunks:
            f.write(
                json.dumps(
                    {"page_content": d.page_content, "metadata": d.metadata},
                    ensure_ascii=False,
                )
                + "\n"
            )
    print(f"ğŸ“ chunks áƒ¨áƒ”áƒœáƒáƒ®áƒ£áƒšáƒ˜áƒ: {out_path}")


def main():
    if not os.getenv("GOOGLE_API_KEY"):
        print("âŒ GOOGLE_API_KEY áƒ•áƒ”áƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ .env-áƒ¨áƒ˜!")
        print("ğŸ‘‰ áƒ“áƒáƒáƒ›áƒáƒ¢áƒ” .env áƒ¤áƒáƒ˜áƒšáƒ¨áƒ˜: GOOGLE_API_KEY=...")
        return

    raw_docs = load_raw_docs()
    if not raw_docs:
        print("âŒ áƒ“áƒáƒ¡áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ”áƒšáƒ˜ áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜ áƒáƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ.")
        print("ğŸ‘‰ áƒ¯áƒ”áƒ  áƒ’áƒáƒ£áƒ¨áƒ•áƒ˜: python src/download_data.py")
        return

    print(f"ğŸ“„ áƒ©áƒáƒ˜áƒ¢áƒ•áƒ˜áƒ áƒ—áƒ {len(raw_docs)} áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ˜")

    langchain_docs: list[Document] = []
    skipped_short = 0

    for item in raw_docs:
        if not isinstance(item, dict):
            continue

        title = (item.get("title") or item.get("name") or "").strip()
        url = (item.get("url") or "").strip()
        doc_id = item.get("id")
        uuid = item.get("uuid")

        raw_html = (
            item.get("text_html")
            or item.get("additionalDescription")
            or item.get("description")
            or item.get("text")
            or ""
        )

        text = clean_html(raw_html)
        content = f"{title}\n\n{text}".strip()

        if len(content) < MIN_CONTENT_LEN:
            skipped_short += 1
            continue

        langchain_docs.append(
            Document(
                page_content=content,
                metadata={
                    "source": url,
                    "title": title,
                    "doc_id": str(doc_id) if doc_id is not None else "",
                    "uuid": str(uuid) if uuid is not None else "",
                    "createDate": str(item.get("createDate") or ""),
                    "updateDate": str(item.get("updateDate") or ""),
                    "species": str(item.get("species") or ""),
                },
            )
        )

    if not langchain_docs:
        print("âŒ áƒ•áƒ”áƒ áƒªáƒ”áƒ áƒ—áƒ˜ áƒ•áƒáƒšáƒ˜áƒ“áƒ£áƒ áƒ˜ áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ˜ áƒ•áƒ”áƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ.")
        return

    print(f"âœ… áƒ•áƒáƒšáƒ˜áƒ“áƒ£áƒ áƒ˜ áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜: {len(langchain_docs)} | áƒ’áƒáƒ›áƒáƒ¢áƒáƒ•áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ›áƒáƒ™áƒšáƒ”: {skipped_short}")

    print("âœ‚ï¸ áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜áƒ¡ áƒ“áƒáƒ­áƒ áƒ chunks-áƒ”áƒ‘áƒáƒ“...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    chunks = splitter.split_documents(langchain_docs)
    print(f"ğŸ“¦ chunks áƒ áƒáƒáƒ“áƒ”áƒœáƒáƒ‘áƒ: {len(chunks)}")

    # Hybrid-áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡ chunks áƒ¤áƒáƒ˜áƒšáƒ¨áƒ˜ áƒ¨áƒ”áƒœáƒáƒ®áƒ•áƒ
    export_chunks_jsonl(chunks, CHUNKS_FILE)

    embeddings = GoogleGenerativeAIEmbeddings(model="gemini-embedding-001")

    # Rebuild Chroma
    if os.path.exists(CHROMA_PATH):
        shutil.rmtree(CHROMA_PATH)
        print("ğŸ§¹ áƒ«áƒ•áƒ”áƒšáƒ˜ chroma_db áƒ¬áƒáƒ˜áƒ¨áƒáƒšáƒ")

    print("ğŸ’¾ áƒ©áƒáƒ¬áƒ”áƒ áƒ ChromaDB-áƒ¨áƒ˜...")
    Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=CHROMA_PATH,
    )

    print("ğŸ‰ áƒ“áƒáƒ¡áƒ áƒ£áƒšáƒ“áƒ! áƒ‘áƒáƒ–áƒ áƒ¨áƒ”áƒ˜áƒ¥áƒ›áƒœáƒ:", CHROMA_PATH)


if __name__ == "__main__":
    main()